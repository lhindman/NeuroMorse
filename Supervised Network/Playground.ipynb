{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Morse code dataset.\n",
    "Morse_Dict = {\"a\":'.-',\n",
    "              \"b\":'-...',\n",
    "              \"c\":'-.-.',\n",
    "              \"d\":'-..',\n",
    "              \"e\":'.',\n",
    "              \"f\":'..-.',\n",
    "              \"g\":'--.',\n",
    "              \"h\":'....',\n",
    "              \"i\":'..',\n",
    "              \"j\":'.---',\n",
    "              \"k\":'-.-',\n",
    "              \"l\":'.-..',\n",
    "              \"m\":'--',\n",
    "              \"n\":'-.',\n",
    "              \"o\":'---',\n",
    "              \"p\":'.--.',\n",
    "              \"q\":'--.-',\n",
    "              \"r\":'.-.',\n",
    "              \"s\":'...',\n",
    "              \"t\":'-',\n",
    "              \"u\":'..-',\n",
    "              \"v\":'...-',\n",
    "              \"w\":'.--',\n",
    "              \"x\":'-..-',\n",
    "              \"y\":'-.--',\n",
    "              \"z\":'--..',\n",
    "              \"1\":'.----',\n",
    "              \"2\":'..---',\n",
    "              \"3\":'...--',\n",
    "              \"4\":'....-',\n",
    "              \"5\":'.....',\n",
    "              \"6\":'-....',\n",
    "              \"7\":'--...',\n",
    "              \"8\":'---..',\n",
    "              \"9\":'----.',\n",
    "              \"0\":'-----',\n",
    "              \".\":'.-.-.-',\n",
    "              \",\":'--..--',\n",
    "              \"?\":'..--..',\n",
    "              \":\":'---...',\n",
    "              \"'\":'.----.',\n",
    "              \"-\":'-....-',\n",
    "              \"/\":'-..-.',\n",
    "              \"(\":'-.--.',\n",
    "              \")\":'-.--.-',\n",
    "              \"\\\"\":'.-..-.',\n",
    "              \"=\":'-...-',\n",
    "              \";\":'-.-.-.',\n",
    "              '$':'...-..-'}\n",
    "\n",
    "\n",
    "space = 5 #Time between dots and dashes.\n",
    "letter_space = 10 #Time between letters\n",
    "word_space = 15 #Time between consecutive words\n",
    "\n",
    "\n",
    "SpikeArray = []\n",
    "New_Dataset = []\n",
    "SpikeDict = {}\n",
    "\n",
    "\n",
    "#Convert each Morse character into spike array\n",
    "for key in Morse_Dict.keys():\n",
    "    time = 0\n",
    "    b= []\n",
    "    for i, ch in enumerate(Morse_Dict[key]):\n",
    "        if ch == '.':\n",
    "            channel = 0\n",
    "        elif ch =='-':\n",
    "            channel = 1\n",
    "        b.append((time,channel,1))\n",
    "        time = time+space+1\n",
    "    SpikeArray = np.array(b,dtype = [('t','<f4'),('x','<f4'),('p','<f4')])\n",
    "    New_Dataset.append((SpikeArray,key))\n",
    "    SpikeDict[key] = SpikeArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[( 0., 0., 1.) ( 6., 0., 1.) (12., 0., 1.) (18., 1., 1.) (24., 0., 1.)\n",
      " (30., 0., 1.) (36., 1., 1.)]\n",
      "dict_keys(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '.', ',', '?', ':', \"'\", '-', '/', '(', ')', '\"', '=', ';', '$'])\n"
     ]
    }
   ],
   "source": [
    "print(SpikeArray)\n",
    "print(SpikeDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Morse Spike Dictionary\n",
    "Morse_Spike_Dict = {}\n",
    "for key in Morse_Dict.keys():\n",
    "    time = 0\n",
    "    b= []\n",
    "    for i,ch in enumerate(Morse_Dict[key]):\n",
    "        if ch == '.':\n",
    "            channel = 0\n",
    "        elif ch =='-':\n",
    "            channel = 1\n",
    "        b.append((time,channel,1))\n",
    "        time = time+space+1\n",
    "    SpikeArray = np.array(b,dtype = [('t','<f4'),('x','<f4'),('p','<f4')])\n",
    "    Morse_Spike_Dict[key] = SpikeArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '.', ',', '?', ':', \"'\", '-', '/', '(', ')', '\"', '=', ';', '$'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Morse_Spike_Dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of top 50 words        \n",
    "Top50List = ['the',\n",
    "             'be',\n",
    "             'to',\n",
    "             'of',\n",
    "             'and',\n",
    "             'a',\n",
    "             'in',\n",
    "             'that',\n",
    "             'have',\n",
    "             'i',\n",
    "             'it',\n",
    "             'for',\n",
    "             'not',\n",
    "             'on',\n",
    "             'with',\n",
    "             'he',\n",
    "             'as',\n",
    "             'you',\n",
    "             'do',\n",
    "             'at',\n",
    "             'this',\n",
    "             'but',\n",
    "             'his',\n",
    "             'by',\n",
    "             'from',\n",
    "             'they',\n",
    "             'we',\n",
    "             'say',\n",
    "             'her',\n",
    "             'she',\n",
    "             'or',\n",
    "             'an',\n",
    "             'will',\n",
    "             'my',\n",
    "             'one',\n",
    "             'all',\n",
    "             'would',\n",
    "             'there',\n",
    "             'their',\n",
    "             'what',\n",
    "             'so',\n",
    "             'up',\n",
    "             'out',\n",
    "             'if',\n",
    "             'about',\n",
    "             'who',\n",
    "             'get',\n",
    "             'which',\n",
    "             'go',\n",
    "             'me']\n",
    "WordDataset = []\n",
    "\n",
    "for idx,word in enumerate(Top50List):\n",
    "    time = 0\n",
    "    list = []\n",
    "    for i,ch in enumerate(word):\n",
    "        Spikes = np.copy(SpikeDict[ch])\n",
    "        Spikes['t'] += time \n",
    "        time = Spikes[-1][0]+(1+letter_space)\n",
    "        list.append(Spikes)\n",
    "\n",
    "    WholeArray = np.concatenate(list)\n",
    "    WordDataset.append((WholeArray,word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSpikeDataset = []\n",
    "training_labels = []\n",
    "num_channels = 2\n",
    "# Map each word to its index\n",
    "word_to_index = {word: idx for idx, word in enumerate(Top50List)}\n",
    "\n",
    "for i in range(50):\n",
    "    data, label = WordDataset[i]\n",
    "    training_labels.append(word_to_index.get(label))\n",
    "    data_neuro = torch.zeros((int(data[-1][0])+1+word_space, num_channels))\n",
    "    for idx in data: \n",
    "        # idx[0] = spike time\n",
    "        # idx[1] = 0 if dot 1 if dash\n",
    "        data_neuro[int(idx[0]),int(idx[1])] = 1\n",
    "    TrainSpikeDataset.append(data_neuro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([( 0., 1., 1.), ( 6., 1., 1.), (17., 0., 1.)],\n",
       "       dtype=[('t', '<f4'), ('x', '<f4'), ('p', '<f4')]),\n",
       " 'me')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordDataset[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test dataset (from corpus) but not loading all because it's slow :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'corpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load a little bit of the corpus (test data)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcorpus.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     corpus = f.read().lower().split()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Select a random subset of words from the corpus\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/NeuroMorse/lib/python3.11/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'corpus.txt'"
     ]
    }
   ],
   "source": [
    "# Load a little bit of the corpus (test data)\n",
    "with open('../data/corpus.txt', 'r', encoding='utf8') as f:\n",
    "    corpus = f.read().lower().split()\n",
    "\n",
    "# Select a random subset of words from the corpus\n",
    "subset_size = 100000  # Adjust this number as needed\n",
    "# subset_indices = random.sample(range(len(corpus)), subset_size)\n",
    "test_subset = corpus[0:subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a translation table\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Remove punctuation from each word\n",
    "cleaned_test_subset = [word.translate(translator) for word in test_subset]\n",
    "\n",
    "# Remove any empty strings resulting from removing punctuation-only words\n",
    "cleaned_test_subset = [word for word in cleaned_test_subset if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test contains:', len(cleaned_test_subset), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (same as in the training script)\n",
    "num_channels = 2  # Dots and dashes\n",
    "word_space = 15  # Spacing between words\n",
    "letter_space = 10  # Spacing between letters\n",
    "\n",
    "# Map each word to its index or OOV label\n",
    "word_to_index = {word: idx for idx, word in enumerate(Top50List)}\n",
    "OOV_label = 50  # Label for OOV words\n",
    "\n",
    "# Create spike trains for each word in the test subset\n",
    "TestSpikeDataset = []\n",
    "test_labels = []\n",
    "\n",
    "for word in cleaned_test_subset:\n",
    "    # Initialize spike train data and label\n",
    "    data = []\n",
    "    label = word_to_index.get(word, OOV_label)  # Assign label or OOV label\n",
    "    time = 0\n",
    "\n",
    "    # Generate spike train for the word\n",
    "    for ch in word:\n",
    "        if ch in Morse_Spike_Dict:\n",
    "            # Retrieve the spike train for the character\n",
    "            char_spikes = np.copy(Morse_Spike_Dict[ch])\n",
    "\n",
    "            # Adjust the spike times to account for the current time\n",
    "            char_spikes['t'] += time\n",
    "\n",
    "            # Convert to the desired structured array format\n",
    "            char_spikes_t_x = np.zeros(len(char_spikes), dtype=[('t', '<f4'), ('x', '<f4')])\n",
    "            char_spikes_t_x['t'] = char_spikes['t']\n",
    "            char_spikes_t_x['x'] = char_spikes['x']\n",
    "\n",
    "            # Append the character's spike train to the word's data\n",
    "            data.extend(char_spikes_t_x)\n",
    "\n",
    "            # Update time for the next character\n",
    "            time = char_spikes['t'][-1] + letter_space\n",
    "        else:\n",
    "            # Skip characters not in Morse_Spike_Dict\n",
    "            continue\n",
    "\n",
    "    # Add spacing for the next word\n",
    "    if data:\n",
    "        # Append the word's spike train to the dataset\n",
    "        data = np.array(data, dtype=[('t', '<f4'), ('x', '<f4')])\n",
    "\n",
    "        # Create a binary spike train tensor\n",
    "        max_time = int(data[-1]['t']) + 1 + word_space  # Add word spacing\n",
    "        data_neuro = torch.zeros((max_time, num_channels))  # Time x Channels\n",
    "\n",
    "        # Populate the spike train tensor\n",
    "        for idx in data:\n",
    "            data_neuro[int(idx['t']), int(idx['x'])] = 1\n",
    "\n",
    "        # Append the processed word's spike train and label\n",
    "        TestSpikeDataset.append(data_neuro)\n",
    "        test_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Vocabulary (OOV) Handling\n",
    "\n",
    "Below is an optional codeblock for if OOV is desired. My thoughts here would be to have one extra neuron that handles anytime the network sees something that is not from the top 50 words. This is a slightly more complex problem, so I trained just using the top 50 in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find all indices where the label is 50 (OOV)\n",
    "# oov_indices = [i for i, label in enumerate(test_labels) if label == 50]\n",
    "\n",
    "# print(f\"Total OOV samples available: {len(oov_indices)}\")\n",
    "\n",
    "# # Desired number of OOV samples to add\n",
    "# desired_oov_samples = 1\n",
    "\n",
    "# # Check if enough OOV samples are available\n",
    "# if len(oov_indices) < desired_oov_samples:\n",
    "#     print(f\"Only {len(oov_indices)} OOV samples available. Selecting all available samples.\")\n",
    "#     selected_oov_indices = oov_indices  # Select all available\n",
    "# else:\n",
    "#     # Randomly select 10 unique indices\n",
    "#     selected_oov_indices = random.sample(oov_indices, desired_oov_samples)\n",
    "\n",
    "# print(f\"Selected OOV sample indices: {selected_oov_indices}\")\n",
    "\n",
    "# # Extract spike trains for selected OOV samples\n",
    "# selected_oov_spike_trains = [TestSpikeDataset[i] for i in selected_oov_indices]\n",
    "\n",
    "# # Extract labels (all should be 50)\n",
    "# selected_oov_labels = [test_labels[i] for i in selected_oov_indices]\n",
    "\n",
    "# # Verify extraction\n",
    "# print(f\"Number of selected OOV spike trains: {len(selected_oov_spike_trains)}\")\n",
    "# print(f\"Corresponding labels: {selected_oov_labels}\")\n",
    "\n",
    "# Append OOV spike trains to TrainSpikeDataset\n",
    "# TrainSpikeDataset += selected_oov_spike_trains\n",
    "# training_labels += selected_oov_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding so that all the dot/dash spike trains are the same length (this makes input much easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum length of spike trains\n",
    "max_length = max(tensor.shape[0] for tensor in TrainSpikeDataset)\n",
    "\n",
    "# Pad all tensors to max_length\n",
    "for i in range(len(TrainSpikeDataset)):\n",
    "    padding = max_length - TrainSpikeDataset[i].shape[0]\n",
    "    TrainSpikeDataset[i] = F.pad(TrainSpikeDataset[i], (0, 0, 0, padding), mode='constant', value=0)  # Pad at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(tensor.shape[0] for tensor in TestSpikeDataset)\n",
    "\n",
    "# Pad all tensors to max_length\n",
    "for i in range(len(TestSpikeDataset)):\n",
    "    padding = max_length - TestSpikeDataset[i].shape[0]\n",
    "    TestSpikeDataset[i] = F.pad(TestSpikeDataset[i], (0, 0, 0, padding), mode='constant', value=0)  # Pad at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all spike train tensors into a single tensor\n",
    "inputs = torch.stack(TrainSpikeDataset)  # Shape: [num_words, max_length, num_channels]\n",
    "test_inputs = torch.stack(TestSpikeDataset) \n",
    "\n",
    "\n",
    "# Convert labels to a torch tensor\n",
    "labels = torch.tensor(training_labels)  # Shape: [num_words]\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes and sample data\n",
    "print(\"Inputs shape:\", inputs.shape)  # Should be [num_words, max_length, num_channels]\n",
    "print(\"Labels shape:\", labels.shape)  # Should be [num_words]\n",
    "print(\"Sample input tensor:\", inputs[0])  # First word's spike train\n",
    "print(\"Sample label:\", labels[1])  # Corresponding label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "spike_grad = surrogate.fast_sigmoid(slope=15)\n",
    "beta = 0.8\n",
    "num_channels = 2  # Dots and dashes\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 2000\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# --- Step 2: Define the SNN Model ---\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_channels, 128)  # Input: num_channels, Output: 128\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad, learn_threshold=True, learn_beta=True)\n",
    "        self.fc2 = nn.Linear(128, 256)  # Input: 128, Output: 64\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad, learn_threshold=True, learn_beta=True)\n",
    "        self.fc3 = nn.Linear(256, 50)  # Output: 50 classes\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad, learn_threshold=True, learn_beta=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        spk1_rec, spk2_rec, spk3_rec = [], [], []\n",
    "\n",
    "        for t in range(x.size(1)):  # Iterate through time steps\n",
    "            cur1 = self.fc1(x[:, t, :])  # Linear layer\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)  # Spiking activation\n",
    "            spk1_rec.append(spk1)\n",
    "\n",
    "            cur2 = self.fc2(spk1)  # Linear layer\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)  # Spiking activation\n",
    "            spk2_rec.append(spk2)\n",
    "\n",
    "            cur3 = self.fc3(spk2)  # Linear layer\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)  # Spiking activation\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "        spk3_rec = torch.stack(spk3_rec, dim=1)\n",
    "        return spk3_rec.mean(dim=1)  # Aggregate spikes over time\n",
    "\n",
    "# Initialize the model\n",
    "model = Net()\n",
    "\n",
    "# --- Step 3: Define Training Pipeline ---\n",
    "criterion = nn.CrossEntropyLoss()  # Loss for classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# --- Step 4: Evaluation ---\n",
    "def evaluate_model(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model on the training set (or validation/test set if available)\n",
    "accuracy = evaluate_model(train_loader)\n",
    "print(f\"Train accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the network\n",
    "\n",
    "First, we test just using the filtered corpus (no punctuation) that we have created. But of course this is a little janky because we will have OOV words that we have not handled at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = evaluate_model(test_loader)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is testing using the vocabulary but with any OOV words removed. This still feels somewhat wrong though because all this data is essentially in the exact same form as the training data was, there is just an imbalance in terms of how often a word is seen. I think we may need to dig deeper into what architecture is appropriate here and what the ultimate goal is. For example, should we rather be doing latency coding? Where the first spike to fire corresponds to a class as was more so what Ben was doing in his STDP example. If we go this direction though we would have to rethink training, as I don't think taking a classification type problem then moving to a continuous stream of data makes sense. We have to match training and test more closely. Also, ultimately latency coding is a trickier problem.\n",
    "\n",
    "In general, this is a tricky problem to approach since we only have 50 training examples which is very little data for classification. Next steps could be to augment the dataset with noise (as Ben has already done, but probably more noisy data), then align on network architecture/what kind of task we want to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask where labels are not equal to 50\n",
    "non_oov_mask = test_labels != 50\n",
    "\n",
    "# Apply the mask to filter test_inputs and test_labels\n",
    "filtered_test_inputs = test_inputs[non_oov_mask]\n",
    "filtered_test_labels = test_labels[non_oov_mask]\n",
    "\n",
    "# Create a new TensorDataset with the filtered data\n",
    "filtered_test_dataset = TensorDataset(filtered_test_inputs, filtered_test_labels)\n",
    "\n",
    "# Create a new DataLoader for the filtered test dataset\n",
    "filtered_test_loader = DataLoader(filtered_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_accuracy = evaluate_model(filtered_test_loader)\n",
    "print(f\"Filtered Test accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert filtered_test_labels to a NumPy array for easier manipulation\n",
    "labels = filtered_test_labels.numpy()\n",
    "\n",
    "# Count the frequency of each label from 0 to 49\n",
    "counts = np.bincount(labels, minlength=50)\n",
    "\n",
    "# Define label range\n",
    "label_range = range(50)  # Labels 0 to 49\n",
    "\n",
    "# Create a figure and axis\n",
    "plt.figure(figsize=(15, 7))  # Adjust the size as needed\n",
    "\n",
    "# Create the bar chart\n",
    "plt.bar(label_range, counts, color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Label', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Frequency of Each Label in Filtered Test Set', fontsize=16)\n",
    "\n",
    "# Set x-ticks to show every label (0-49)\n",
    "plt.xticks(label_range, rotation=90)  # Rotate labels for better readability\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout to prevent clipping of tick-labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroMorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
